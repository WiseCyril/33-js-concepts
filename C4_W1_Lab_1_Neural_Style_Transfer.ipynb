{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "C4_W1_Lab_1_Neural_Style_Transfer.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WiseCyril/33-js-concepts/blob/master/C4_W1_Lab_1_Neural_Style_Transfer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chuVvdHj3qgW"
      },
      "source": [
        "# Ungraded Lab: Neural Style Transfer\n",
        "\n",
        "This lab will demonstrate neural style transfer using the techniques discussed in class. You will revisit this again after Lesson 2 of this week's lecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eqxUicSPUOP6"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc1OLbOWhPCO"
      },
      "source": [
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "import IPython.display as display_obj\n",
        "from random import randint\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib as mpl\n",
        "mpl.rcParams['figure.figsize'] = (12,12)\n",
        "mpl.rcParams['axes.grid'] = False\n",
        "\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "import time\n",
        "import functools\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0U9It5Ii2Oof"
      },
      "source": [
        "## Download Images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeXebYusyHwC"
      },
      "source": [
        "Download images and choose a style image and a content image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqc0OJHwyFAk"
      },
      "source": [
        "!wget  https://cdn.pixabay.com/photo/2018/07/14/15/27/cafe-3537801_1280.jpg\n",
        "!wget  https://cdn.pixabay.com/photo/2017/02/28/23/00/swan-2107052_1280.jpg\n",
        "!wget  https://i.dawn.com/large/2019/10/5db6a03a4c7e3.jpg\n",
        "!wget  https://cdn.pixabay.com/photo/2015/09/22/12/21/rudolph-951494_1280.jpg\n",
        "!wget https://cdn.pixabay.com/photo/2015/10/13/02/59/animals-985500_1280.jpg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xE4Yt8nArTeR"
      },
      "source": [
        "## Visualize the input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klh6ObK2t_vH"
      },
      "source": [
        "Define a function to load an image and limit its maximum dimension to 512 pixels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GM6VEGrGLh62"
      },
      "source": [
        "def tensor_to_image(tensor):\n",
        "  tensor_shape = tf.shape(tensor)\n",
        "  number_elem_shape = tf.shape(tensor_shape)\n",
        "  if number_elem_shape > 3:\n",
        "    assert tensor_shape[0] == 1\n",
        "    tensor = tensor[0]\n",
        "  return tf.keras.preprocessing.image.array_to_img(tensor) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbf5_bq_3qgf"
      },
      "source": [
        "### Load and preprocess the image\n",
        "\n",
        "This code is given to you.  \n",
        "- You will use preprocess_image in your code later in this assignment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3TLljcwv5qZs"
      },
      "source": [
        "def load_img(path_to_img):\n",
        "  max_dim = 512\n",
        "  image = tf.io.read_file(path_to_img)\n",
        "  image = tf.image.decode_jpeg(image)\n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "\n",
        "  shape = tf.shape(image)[:-1]\n",
        "  shape = tf.cast(tf.shape(image)[:-1], tf.float32)\n",
        "  long_dim = max(shape)\n",
        "  scale = max_dim / long_dim\n",
        "\n",
        "  new_shape = tf.cast(shape * scale, tf.int32)\n",
        "\n",
        "  image = tf.image.resize(image, new_shape)\n",
        "  image = image[tf.newaxis, :]\n",
        "  image = tf.image.convert_image_dtype(image, tf.uint8)\n",
        "  return image\n",
        "\n",
        "def preprocess_image(image):\n",
        "    image = tf.cast(image, dtype=tf.float32)\n",
        "    image = tf.keras.applications.vgg19.preprocess_input(image)\n",
        "\n",
        "    return image\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2yAlRzJZrWM3"
      },
      "source": [
        "Create a simple function to display an image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cBX-eNT8PAK_"
      },
      "source": [
        "def imshow(image, title=None):\n",
        "  if len(image.shape) > 3:\n",
        "    image = tf.squeeze(image, axis=0)\n",
        "\n",
        "  plt.imshow(image)\n",
        "  if title:\n",
        "    plt.title(title)\n",
        "    \n",
        "    \n",
        "def show_images_with_objects(images, titles=[]):\n",
        "\n",
        "  if len(images) != len(titles):\n",
        "    return\n",
        "\n",
        "  plt.figure(figsize=(20, 12))\n",
        "  for idx, (image, title) in enumerate(zip(images, titles)):\n",
        "    plt.subplot(1, len(images), idx + 1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    imshow(image, title)\n",
        "    \n",
        "    \n",
        "def load_images(content_path, style_path):\n",
        "  content_image = load_img(\"{}\".format(content_path))\n",
        "  style_image = load_img(\"{}\".format(style_path))\n",
        "\n",
        "  return content_image, style_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jt3i3RRrJiOX"
      },
      "source": [
        "## Build the model "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48cNdd0N3qgg"
      },
      "source": [
        "First, download the VGG19 model so that you can inspect the layers that are available for you to use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7JEI_tdT3qgg"
      },
      "source": [
        "tmp_vgg = tf.keras.applications.vgg19.VGG19()\n",
        "tmp_vgg.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wt-tASys0eJv"
      },
      "source": [
        "Choose intermediate layers from the network to represent the style and content of the image:\n",
        "\n",
        "- For the content layer, please use the second convolutional layer of the last convolutional block (just one layer)\n",
        "- For the style layers, please use the first layer of each convolutional block."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ArfX_6iA0WAX"
      },
      "source": [
        "# Content layer where will pull our feature maps\n",
        "\n",
        "# Fill in the list for content_layers\n",
        "content_layers = ['block5_conv2'] \n",
        "\n",
        "# Style layer of interest\n",
        "style_layers = ['block1_conv1', \n",
        "                'block2_conv1', \n",
        "                'block3_conv1', \n",
        "                'block4_conv1', \n",
        "                'block5_conv1'] \n",
        "\n",
        "# combine the two lists (put the style layers before the content layers)\n",
        "output_layers = style_layers + content_layers \n",
        "\n",
        "num_content_layers = len(content_layers)\n",
        "num_style_layers = len(style_layers)\n",
        "num_output_layers = len(output_layers)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKw5AaWB3qgg"
      },
      "source": [
        "Define your model to take the same input as the standard vgg19 model, and output just the selected content and style layers."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfec6MuMAbPx"
      },
      "source": [
        "def vgg_model(layer_names):\n",
        "  \"\"\" Creates a vgg model that returns a list of intermediate output values.\n",
        "  \n",
        "  args:\n",
        "    layer_names: a list of strings, representing the names of the desired content and style layers\n",
        "    \n",
        "  returns:\n",
        "    A model that takes the regular vgg19 input and outputs just the content and style layers.\n",
        "  \n",
        "  \"\"\"\n",
        "\n",
        "  # Load the the pretrained VGG, trained on imagenet data\n",
        "  vgg = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet')\n",
        "\n",
        "  # Freeze the weights of the model's layers (make them not trainable)\n",
        "  vgg.trainable = False\n",
        "  \n",
        "  # Create a list of layer objects that are specified by layer_names\n",
        "  outputs = [vgg.get_layer(name).output for name in layer_names]\n",
        "\n",
        "  # Create the model that outputs content and style layers only\n",
        "  model = tf.keras.Model(inputs=vgg.input, outputs=outputs)\n",
        "\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JEROBesg3qgh"
      },
      "source": [
        "Create an instance of the content and style model using the function that you just defined"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7RJLgwAn3qgi"
      },
      "source": [
        "vgg = vgg_model(output_layers) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jbaIvZf5wWn_"
      },
      "source": [
        "### Calculate style loss\n",
        "\n",
        "The style loss is the average of the squared differences between the features and targets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nv8hZU0oKIm_"
      },
      "source": [
        "def get_style_loss(features, targets):\n",
        "  \"\"\"Expects two images of dimension h, w, c\n",
        "  args:\n",
        "    features: tensor with shape: (height, width, channels)\n",
        "    targets: tensor with shape: (height, width, channels)\n",
        "  \"\"\"\n",
        "    \n",
        "  # Calculate the style style loss\n",
        "  style_loss = tf.reduce_mean(tf.square(features - targets))\n",
        "    \n",
        "  return style_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRzTkttk3qgi"
      },
      "source": [
        "### Calculate content loss\n",
        "\n",
        "Calculate the content loss as the average squared difference between the features and targets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et8M1lOgKL8o"
      },
      "source": [
        "def get_content_loss(features, targets):\n",
        "  \"\"\"Expects two images of dimension h, w, c\n",
        "  args:\n",
        "    features: tensor with shape: (height, width, channels)\n",
        "    targets: tensor with shape: (height, width, channels)\n",
        "  \"\"\"\n",
        "    \n",
        "  # Calculate the style style loss\n",
        "  content_loss = tf.reduce_mean(tf.square(features - targets)) # @REPLACE content_loss = None\n",
        "    \n",
        "  return content_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTCuv2663qgi"
      },
      "source": [
        "### Calculate the gram matrix\n",
        "\n",
        "Use `tf.linalg.einsum` to calculate the gram matrix for an input tensor.\n",
        "- In addition, calculate the scaling factor `num_locations` and divide the gram matrix calculation by `num_locations`.\n",
        "\n",
        "$$ \\text{num locations} = height \\times width $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAy1iGPdoEpZ"
      },
      "source": [
        "def gram_matrix(input_tensor):\n",
        "  \"\"\" Calculates the gram matrix and divides by the number of locations\n",
        "  args:\n",
        "    input_tensor: tensor of shape (batch, height, width, channels)\n",
        "    \n",
        "    \n",
        "  return:\n",
        "    scaled_gram: gram matrix divided by the number of locations\n",
        "  \"\"\"\n",
        "\n",
        "  # Calculate the gram matrix of the input tensor\n",
        "  gram = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor) \n",
        "\n",
        "  # get the height and width of the input tensor\n",
        "  input_shape = tf.shape(input_tensor) \n",
        "  height = input_shape[1] \n",
        "  width = input_shape[2] \n",
        "\n",
        "  # Get the number of locations (height times width), and cast it as a tf.float32\n",
        "  num_locations = tf.cast(height * width, tf.float32)\n",
        "\n",
        "  # Scale the gram matrix by dividing by the number of locations\n",
        "  scaled_gram = gram / num_locations\n",
        "    \n",
        "  # return the scaled gram matrix\n",
        "  return scaled_gram"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hn7HSoNu3qgj"
      },
      "source": [
        "### Get the style image features\n",
        "\n",
        "Given the style image as input, you'll get the style features of the custom vgg model that you just created using `vgg_model()`.\n",
        "- You'll first preprocess the image using the given `preprocess_image` function.\n",
        "- You'll then get the outputs of the vgg model.\n",
        "- From the outputs, just get the style feature layers and not the content feature layer.\n",
        "\n",
        "You can run the following code to check the order of the layers in your custom vgg model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99ya0QpO3qgj"
      },
      "source": [
        "tmp_layer_list = [layer.output for layer in vgg.layers]\n",
        "tmp_layer_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqA0ffs13qgk"
      },
      "source": [
        "- For each style layer, calculate the gram matrix.  Store these results in a list and return it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YzTK5qzG_MKh"
      },
      "source": [
        "def get_style_image_features(image):  \n",
        "  \"\"\" Get the style image features\n",
        "  \n",
        "  args:\n",
        "    image: an input image\n",
        "    \n",
        "  return:\n",
        "    gram_style_features: the style features as gram matrices\n",
        "  \"\"\"\n",
        "  # preprocess the image using the given preprocessing function\n",
        "  preprocessed_style_image = preprocess_image(image) \n",
        "\n",
        "  # get the outputs from the custom vgg model that you created using vgg_model()\n",
        "  outputs = vgg(preprocessed_style_image) \n",
        "\n",
        "  # Get just the style feature layers (exclude the content layer)\n",
        "  style_outputs = outputs[:num_style_layers] \n",
        "\n",
        "  # for each style layer, calculate the gram matrix for that layer and store these results in a list\n",
        "  gram_style_features = [gram_matrix(style_layer) for style_layer in style_outputs] \n",
        "\n",
        "  return gram_style_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LwUzBs023qgk"
      },
      "source": [
        "### Get content image features\n",
        "\n",
        "Given the content image as input, you'll get the style features (not the content features) of the content image.\n",
        "- You can follow a similar process as you did with `get_style_image_features`.\n",
        "- For the content image, you will not calculate the gram matrix of these style features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7rq02U9_a6L"
      },
      "source": [
        "def get_content_image_features(image):\n",
        "\n",
        "  # preprocess the image\n",
        "  preprocessed_content_image = preprocess_image(image)\n",
        "    \n",
        "  # get the outputs from the vgg model\n",
        "  outputs = vgg(preprocessed_content_image) \n",
        "\n",
        "  # get the style layers of the outputs (excluding the content layer output)\n",
        "  style_outputs = outputs[num_style_layers:]\n",
        "\n",
        "  # return the style layer outputs of the content image\n",
        "  return style_outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mB9ZCNbq3qgk"
      },
      "source": [
        "### Calculate the style and content loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q20XhIHnotQA"
      },
      "source": [
        "def get_style_content_loss(style_targets, style_outputs, content_targets, content_outputs, style_weight, content_weight):\n",
        "    \n",
        "  # Sum of the style losses\n",
        "  style_loss = tf.add_n([ get_style_loss(style_output, style_target)\n",
        "                           for style_output, style_target in zip(style_outputs, style_targets)])\n",
        "  \n",
        "  # Sum up the content losses\n",
        "  content_loss = tf.add_n([get_content_loss(content_output, content_target)\n",
        "                           for content_output, content_target in zip(content_outputs, content_targets)])\n",
        "\n",
        "  # scale the style loss by multiplying by the style weight and dividing by the number of style layers\n",
        "  style_loss = style_loss * style_weight / num_style_layers \n",
        "\n",
        "  # scale the content loss by multiplying by the content weight and dividing by the number of content layers\n",
        "  content_loss =content_loss * content_weight / num_content_layers \n",
        "    \n",
        "  # sum up the style and content losses\n",
        "  total_loss = style_loss + content_loss \n",
        "\n",
        "  # return the total loss\n",
        "  return total_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsTDgnyS3qgl"
      },
      "source": [
        "### Clip the image\n",
        "\n",
        "This is given to you"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdgpTJwL_vE2"
      },
      "source": [
        "def clip_0_1(image):\n",
        "  return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrBIWelC3qgl"
      },
      "source": [
        "### Calculate gradients\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp2g2tI58RI0"
      },
      "source": [
        "def calculate_gradients(image, content_targets, style_targets, style_weight, content_weight):\n",
        "    total_variation_weight = 30\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "      # scale up the image from [0 to 1] to [0 to 255] by multiplying by 255\n",
        "      image = image * 255 \n",
        "        \n",
        "      # get the style image features\n",
        "      style_features = get_style_image_features(image) \n",
        "        \n",
        "      # get the content image features\n",
        "      content_features = get_content_image_features(image) \n",
        "        \n",
        "      # get the style and content loss\n",
        "      loss = get_style_content_loss(style_targets, style_features, content_targets, content_features, style_weight, content_weight) \n",
        "\n",
        "    # calculate gradients of loss with respect to the image\n",
        "    gradients = tape.gradient(loss, image) \n",
        "\n",
        "    # return gradients\n",
        "    return gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-crYKGz3qgl"
      },
      "source": [
        "### Update the image with the style\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-MPRxuGp-5A"
      },
      "source": [
        "def update_image_with_style(image, content_targets, style_targets, optimizer, style_weight, content_weight):\n",
        "\n",
        "  # Calculate gradients using the function that you just defined.\n",
        "  gradients = calculate_gradients(image, content_targets, style_targets, style_weight, content_weight) \n",
        "\n",
        "  # apply the gradients to the given image\n",
        "  optimizer.apply_gradients([(gradients, image)]) \n",
        "\n",
        "  # Clip the image using the given clip_0_1() function\n",
        "  image.assign(clip_0_1(image))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foTOpNNw2Wp2"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0Btr_j9M1gu"
      },
      "source": [
        "def fit_style_transfer(input_image, style_image, optimizer, epochs=1, steps_per_epoch=1, style_weight = 0.01, with_regularization=False):\n",
        "\n",
        "  images = []\n",
        "  import time\n",
        "  start = time.time()\n",
        "\n",
        "  step = 0\n",
        "\n",
        "  #style_weight=1.0\n",
        "  content_weight=1e2\n",
        "\n",
        "  # get the style image features \n",
        "  style_targets = get_style_image_features(style_image)\n",
        "    \n",
        "  # get the content image features\n",
        "  content_targets = get_content_image_features(input_image)\n",
        "\n",
        "\n",
        "  input_image = tf.image.convert_image_dtype(input_image, dtype=tf.float32)\n",
        "  \n",
        "  input_image = tf.Variable(input_image) \n",
        "  images.append(tf.Variable(input_image)) \n",
        "  \n",
        "  for n in range(epochs):\n",
        "    for m in range(steps_per_epoch):\n",
        "      step += 1\n",
        "    \n",
        "      # Update the image with the style using the function that you defined\n",
        "      update_image_with_style(input_image, content_targets, style_targets, optimizer, style_weight, content_weight) \n",
        "    \n",
        "\n",
        "      print(\".\", end='')\n",
        "      if (m + 1) % 10 == 0:\n",
        "        images.append(tf.Variable(input_image))\n",
        "    \n",
        "    display_obj.clear_output(wait=True)\n",
        "    display_image = tensor_to_image(input_image)\n",
        "\n",
        "    \n",
        "    display_obj.display(display_image)\n",
        "    images.append(tf.Variable(input_image))\n",
        "    print(\"Train step: {}\".format(step))\n",
        "  end = time.time()\n",
        "  print(\"Total time: {:.1f}\".format(end-start)) \n",
        "  \n",
        "  return input_image, images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Id4QTQl3qgm"
      },
      "source": [
        "### Load some images to try it out!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcOmo92OyABa"
      },
      "source": [
        "content_image, style_image = load_images(\"swan-2107052_1280.jpg\", \"animals-985500_1280.jpg\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQW1tXYoLbUS"
      },
      "source": [
        "weight =  0.01 \n",
        "adam = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
        "stylized_image, display_images = fit_style_transfer(input_image=content_image, style_image=style_image, optimizer=adam, epochs=10, steps_per_epoch=100, style_weight=weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73Qiu1hyy4W3"
      },
      "source": [
        "# Display Utilities\n",
        "\n",
        "import imageio\n",
        "from IPython.display import display as display_fn\n",
        "from IPython.display import Image\n",
        "\n",
        "def display_gif(GIF_PATH):\n",
        "  with open(GIF_PATH,'rb') as f:\n",
        "    display_fn(Image(data=f.read(), format='png'))\n",
        "\n",
        "def create_gif(images):\n",
        "  GIF_PATH = \"/content/{}.gif\".format(randint(0, 10000))\n",
        "  imageio.mimsave(GIF_PATH, images, fps=1)\n",
        "  return GIF_PATH\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZWFMUQ_wJnWp"
      },
      "source": [
        "# Display GIF of Intermedite Outputs\n",
        "gif_images = [np.squeeze(image.numpy(), axis=0) for image in display_images]\n",
        "gif_path = create_gif(gif_images)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mixNaLAl6FUO"
      },
      "source": [
        "display_gif(gif_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brn35mZu6acj"
      },
      "source": [
        "## End of Lesson 1 ungraded lab\n",
        "\n",
        "Please go back to the classroom and watch lesson 2 regarding the total variation loss. Then you can continue on to the next section below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWVB3anJMY2v"
      },
      "source": [
        "## Total variation loss\n",
        "\n",
        "One downside to this basic implementation is that it produces a lot of high frequency artifacts. Decrease these using an explicit regularization term on the high frequency components of the image. In style transfer, this is often called the *total variation loss*. Let's define the `calculate_gradients()` function again but this time with a regularization parameter to compute the total variation loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmlXkUvk9GGL"
      },
      "source": [
        "def calculate_gradients(image, content_targets, style_targets, style_weight, content_weight, with_regularization=False):\n",
        "    total_variation_weight = 160\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "      # scale up the image from [0 to 1] to [0 to 255] by multiplying by 255\n",
        "      image = image * 255 \n",
        "        \n",
        "      # get the style image features\n",
        "      style_features = get_style_image_features(image) \n",
        "        \n",
        "      # get the content image features\n",
        "      content_features = get_content_image_features(image) \n",
        "        \n",
        "      # get the style and content loss\n",
        "      loss = get_style_content_loss(style_targets, style_features, content_targets, content_features, style_weight, content_weight)\n",
        "\n",
        "      # to take into account the total variation loss (discussed in Lesson 2)\n",
        "      if with_regularization:\n",
        "        loss += total_variation_weight*tf.image.total_variation(image)\n",
        "\n",
        "    # calculate gradients of loss with respect to the image\n",
        "    gradients = tape.gradient(loss, image) \n",
        "\n",
        "    # return gradients\n",
        "    return gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TrAkGDH_U97"
      },
      "source": [
        "# Plot Utilities\n",
        "def high_pass_x_y(image):\n",
        "  x_var = image[:,:,1:,:] - image[:,:,:-1,:]\n",
        "  y_var = image[:,1:,:,:] - image[:,:-1,:,:]\n",
        "\n",
        "  return x_var, y_var\n",
        "\n",
        "def plot_deltas_for_single_image(x_deltas, y_deltas, name=\"Original\", row=1):\n",
        "  plt.figure(figsize=(14,10))\n",
        "  plt.subplot(row,2,1)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "\n",
        "  imshow(clip_0_1(2*y_deltas+0.5), \"Horizontal Deltas: {}\".format(name))\n",
        "\n",
        "  plt.subplot(row,2,2)\n",
        "  plt.yticks([])\n",
        "  plt.xticks([])\n",
        "  \n",
        "  imshow(clip_0_1(2*x_deltas+0.5), \"Vertical Deltas: {}\".format(name))\n",
        "\n",
        "def plot_deltas(original_image_deltas, stylized_image_deltas):\n",
        "  orig_x_deltas, orig_y_deltas = original_image_deltas\n",
        "  \n",
        "  stylized_x_deltas, stylized_y_deltas = stylized_image_deltas\n",
        "\n",
        "  plot_deltas_for_single_image(orig_x_deltas, orig_y_deltas, name=\"Original\")\n",
        "  plot_deltas_for_single_image(stylized_x_deltas, stylized_y_deltas, name=\"Stylized Image\", row=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn67NdjAR2xr"
      },
      "source": [
        "# Display Frequency Variations\n",
        "\n",
        "original_x_deltas, original_y_deltas = high_pass_x_y(tf.image.convert_image_dtype(content_image, dtype=tf.float32))\n",
        "stylized_image_x_deltas, stylized_image_y_deltas = high_pass_x_y(stylized_image)\n",
        "\n",
        "plot_deltas((original_x_deltas, original_y_deltas), (stylized_image_x_deltas, stylized_image_y_deltas))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTessd-DCdcC"
      },
      "source": [
        "## Re-run the optimization\n",
        "\n",
        "Choose a weight for the `total_variation_loss`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a-dPRr8BqexB"
      },
      "source": [
        "variation_model_weight =   0.01\n",
        "\n",
        "stylized_image1, display_images1 = fit_style_transfer(input_image=content_image, style_image=style_image, optimizer=adam, epochs=10, steps_per_epoch=100, with_regularization=True, style_weight=variation_model_weight)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pul5V0ig5PKS"
      },
      "source": [
        "# Display GIF\n",
        "\n",
        "gif_images1 = [np.squeeze(image.numpy(), axis=0) for image in display_images1]\n",
        "gif_path1 = create_gif(gif_images1)\n",
        "display_gif(gif_path1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lla8IAunRviU"
      },
      "source": [
        "# Display Frequency Variations\n",
        "\n",
        "original_x_deltas, original_y_deltas = high_pass_x_y(tf.image.convert_image_dtype(content_image, dtype=tf.float32))\n",
        "stylized_image_x_deltas, stylized_image_y_deltas = high_pass_x_y(stylized_image1)\n",
        "\n",
        "plot_deltas((original_x_deltas, original_y_deltas), (stylized_image_x_deltas, stylized_image_y_deltas))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hS6h-0EaCD_P"
      },
      "source": [
        "show_images_with_objects([style_image, content_image, stylized_image], titles=['Style Image', 'Content Image', 'Stylized Image'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POtMRtWBAz21"
      },
      "source": [
        "show_images_with_objects([style_image, content_image, stylized_image1], titles=['Style Image', 'Content Image', 'Stylized Image'])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}